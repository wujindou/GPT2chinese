# -*- coding: utf-8 -*-
"""gpt2_reward_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pOYsa7B0OXDjHUxd3ikeATKbVoKSdQ_Y
"""

!git clone https://github.com/XiaoMi/nlpcc-2023-shared-task-9.git

!pip install tqdm transformers datasets evaluate rouge-score

import sys
from tqdm import tqdm 
def dfs(start,nums,path,res):
    if len(path) == 2:
        res.append(path[:])
        return
    if start>=len(nums):
        return
    if nums[start] in path:
        return
    path.append(nums[start])
    dfs(start+1,nums,path,res)
    path.pop()
    dfs(start+1,nums,path,res)

def get_two_permutations(nums):
    res = []
    dfs(0,nums,[],res)
    return res

def convert_data(dataset):
  pairs = []
  for d in tqdm(dataset):
    prompt = '用户:'+d['query']+'\n'
    lens = len(d['replys'])
    nums = [i for i in range(lens)]
    permutations = get_two_permutations(nums)
    for permutation in permutations:
        first = d['replys'][permutation[0]]
        second = d['replys'][permutation[1]]
        chosen_summary = ''
        rejected_summary = ''
        pair = {}
        if first['like']-first['dislike'] == second['like'] - second['dislike']:
            continue
        if first['like']-first['dislike']>second['like'] - second['dislike']:
            chosen_summary = '#小爱同学:'+first['reply']
            rejected_summary ='#小爱同学:'+second['reply']
        else:
            chosen_summary = '#小爱同学:'+second['reply']
            rejected_summary ='#小爱同学:'+first['reply']
        pair["chosen"] = prompt + chosen_summary
        pair["rejected"] = prompt + rejected_summary
        pairs.append(pair)

  return pairs

import json 



dataset = []
data_path = './nlpcc-2023-shared-task-9/datasets/'
train_file_name = 'datasets_train.jsonl'
dev_file_name = 'datasets_dev.jsonl'

train_data = [json.loads(lin) for lin in open(data_path+'/'+train_file_name,'r',encoding='utf-8').readlines()]
dev_data = [json.loads(lin) for lin in open(data_path+'/'+dev_file_name,'r',encoding='utf-8').readlines()]

train_pairs = convert_data(train_data)
dev_paris = convert_data(dev_data)

from torch.utils.data import Dataset
from tqdm import tqdm
from transformers import AutoTokenizer, Trainer, TrainingArguments
import torch 

checkpoint = 'uer/gpt2-chinese-cluecorpussmall'
tokenizer = AutoTokenizer.from_pretrained(checkpoint)



class PairwiseDataset(Dataset):
  def __init__(self, pairs, tokenizer, max_length):
    self.chosen_input_ids = []
    self.chosen_attn_masks = []
    self.rejected_input_ids = []
    self.rejected_attn_masks = []
    for pair in tqdm(pairs):
        chosen, rejected = pair["chosen"], pair["rejected"]
        chosen_encodings_dict = tokenizer(
            "[unused1]" + chosen + "[PAD]",
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors="pt",
            add_special_tokens=False
        )
        rejected_encodings_dict = tokenizer(
            "[unused1]" + rejected + "[PAD]",
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors="pt",
            add_special_tokens=False
        )
        if not torch.all(torch.eq(chosen_encodings_dict["input_ids"], rejected_encodings_dict["input_ids"])).item():
            self.chosen_input_ids.append(chosen_encodings_dict["input_ids"])
            self.chosen_attn_masks.append(chosen_encodings_dict["attention_mask"])
            self.rejected_input_ids.append(rejected_encodings_dict["input_ids"])
            self.rejected_attn_masks.append(rejected_encodings_dict["attention_mask"])

  def __len__(self):
    return len(self.chosen_input_ids)

  def __getitem__(self, idx):
    return (
        self.chosen_input_ids[idx],
        self.chosen_attn_masks[idx],
        self.rejected_input_ids[idx],
        self.rejected_attn_masks[idx],
    )

class DataCollatorReward:
  def __call__(self, data):
    batch = {}
    batch["input_ids"] = torch.cat([f[0] for f in data] + [f[2] for f in data])
    batch["attention_mask"] = torch.cat([f[1] for f in data] + [f[3] for f in data])
    batch["labels"] = torch.tensor([0] * len(data) + [1] * len(data))
    return batch


def compute_metrics(eval_preds):
  chosen_end_scores = eval_preds.predictions[0]  # chosen scores
  rejected_end_scores = eval_preds.predictions[1]  # rejected scores
  result = {}
  acc = sum(chosen_end_scores > rejected_end_scores) / len(rejected_end_scores)
  result["accuracy"] = acc

  return result

# tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
print(tokenizer.eos_token)
print(tokenizer.pad_token)

import torch
from torch import nn
from transformers import AutoModelForCausalLM, AutoTokenizer

class GPTRewardModel(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        model = AutoModelForCausalLM.from_pretrained(model_path)
        self.config = model.config
        # `gpt-neo(x)` models use `hidden_size` attribute names instead of `n_embd``
        self.config.n_embd = self.config.hidden_size if hasattr(self.config, "hidden_size") else self.config.n_embd
        self.transformer = model.transformer
        self.v_head = nn.Linear(self.config.n_embd, 1, bias=False)
        self.tokenizer = AutoTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        # self.tokenizer.pad_token ="[SEP]" #self.tokenizer.eos_token
        self.PAD_ID = self.tokenizer(self.tokenizer.pad_token)["input_ids"][0]

    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        mc_token_ids=None,
        labels=None,
        return_dict=False,
        output_attentions=False,
        output_hidden_states=False,
    ):
        loss = None
        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
        )

        hidden_states = transformer_outputs[0]

        rewards = self.v_head(hidden_states).squeeze(-1)
        chosen_end_scores = []
        rejected_end_scores = []

        # Split the inputs and rewards into two parts, chosen and rejected
        assert len(input_ids.shape) == 2
        bs = input_ids.shape[0] // 2
        chosen = input_ids[:bs]
        rejected = input_ids[bs:]
        chosen_rewards = rewards[:bs]
        rejected_rewards = rewards[bs:]

        loss = 0
        inference = False
        for i in range(bs):
            if torch.all(torch.eq(chosen[i], rejected[i])).item():
                c_inds = (chosen[i] == self.PAD_ID).nonzero()
                c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]
                chosen_end_scores.append(chosen_rewards[i, c_ind - 1])
                inference = True
                continue

            # Check if there is any padding otherwise take length of sequence
            c_inds = (chosen[i] == self.PAD_ID).nonzero()
            # print(chosen[i])
            c_ind = c_inds[0].item() if len(c_inds) > 0 else chosen.shape[1]
            # print(rejected[i])
            r_inds = (rejected[i] == self.PAD_ID).nonzero()
            r_ind = r_inds[0].item() if len(r_inds) > 0 else rejected.shape[1]
            end_ind = max(c_ind, r_ind)

            # Retrieve first index where trajectories diverge
            divergence_ind = (chosen[i] != rejected[i]).nonzero()[0]
            assert divergence_ind > 0

            # Index into the correct rewards
            c_truncated_reward = chosen_rewards[i][divergence_ind:end_ind]
            r_truncated_reward = rejected_rewards[i][divergence_ind:end_ind]
            # print(c_truncated_reward)
            # print(r_truncated_reward)

            # Append the last rewards to the list of end scores
            if len(c_truncated_reward)>0:
              chosen_end_scores.append(c_truncated_reward[-1])
              rejected_end_scores.append(r_truncated_reward[-1])

            # Compute loss based on truncated rewards (ignore padding)
            loss += -torch.log(torch.sigmoid(c_truncated_reward - r_truncated_reward)).mean()
        loss = loss / bs

        if not inference:
            chosen_end_scores = torch.stack(chosen_end_scores)
            rejected_end_scores = torch.stack(rejected_end_scores)

        if inference:
            chosen_end_scores = torch.stack(chosen_end_scores)
            return {"chosen_end_scores": chosen_end_scores}

        return {
            "loss": loss,
            "chosen_end_scores": chosen_end_scores,
            "rejected_end_scores": rejected_end_scores,
        }

training_args = TrainingArguments(
        output_dir="rm_checkpoint/",
        num_train_epochs=5,
        logging_steps=10,
        gradient_accumulation_steps=4,
        save_strategy="steps",
        evaluation_strategy="steps",
        per_device_train_batch_size=16,
        per_device_eval_batch_size=1,
        eval_accumulation_steps=1,
        eval_steps=500,
        save_steps=500,
        warmup_steps=100,
        logging_dir="./logs",
        fp16=True,
        # bf16=False,
        learning_rate=1e-5,
        # deepspeed="ds_config_gpt_j.json",
        save_total_limit=1,
)

# Make pairwise datasets for training
max_length = 128
train_dataset = PairwiseDataset(train_pairs, tokenizer, max_length=max_length)
val_dataset = PairwiseDataset(dev_paris, tokenizer, max_length=max_length)

model = GPTRewardModel(checkpoint)

# Freeze the first 70% of the hidden layers of the reward model backbone
layers = model.transformer.h
num_layers = len(layers)
num_unfrozen = int(0.3 * num_layers)
for layer in layers[:-num_unfrozen]:
    layer.requires_grad_(False)

# Create the comparisons datasets
# data_path = "CarperAI/openai_summarize_comparisons"
# train_pairs = create_comparison_dataset(data_path, "train")
# val_pairs = create_comparison_dataset(data_path, "test")



# Create the collator to gather batches of pairwise comparisons
data_collator = DataCollatorReward()

Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_metrics=compute_metrics,
    eval_dataset=val_dataset,
    data_collator=data_collator,
).train()
